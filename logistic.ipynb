{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.6), please consider upgrading to the latest version (0.3.8).\n",
      "Path to dataset files: C:\\Users\\pc\\.cache\\kagglehub\\datasets\\nitishabharathi\\cert-insider-threat\\versions\\1\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"nitishabharathi/cert-insider-threat\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Files: ['email.csv', 'psychometric.csv']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\pc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\pc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score ,precision_score, recall_score, f1_score\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "# List all files in the dataset directory\n",
    "files = os.listdir(path)\n",
    "print(\"Dataset Files:\", files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         id                 date     user       pc  \\\n",
      "0  {R3I7-S4TX96FG-8219JWFF}  01/02/2010 07:11:45  LAP0338  PC-5758   \n",
      "1  {R0R9-E4GL59IK-2907OSWJ}  01/02/2010 07:12:16  MOH0273  PC-6699   \n",
      "2  {G2B2-A8XY58CP-2847ZJZL}  01/02/2010 07:13:00  LAP0338  PC-5758   \n",
      "3  {A3A9-F4TH89AA-8318GFGK}  01/02/2010 07:13:17  LAP0338  PC-5758   \n",
      "4  {E8B7-C8FZ88UF-2946RUQQ}  01/02/2010 07:13:28  MOH0273  PC-6699   \n",
      "\n",
      "                                                  to  \\\n",
      "0  Dean.Flynn.Hines@dtaa.com;Wade_Harrison@lockhe...   \n",
      "1                        Odonnell-Gage@bellsouth.net   \n",
      "2                         Penelope_Colon@netzero.com   \n",
      "3                          Judith_Hayden@comcast.net   \n",
      "4  Bond-Raymond@verizon.net;Alea_Ferrell@msn.com;...   \n",
      "\n",
      "                                cc                          bcc  \\\n",
      "0  Nathaniel.Hunter.Heath@dtaa.com                          NaN   \n",
      "1                              NaN                          NaN   \n",
      "2                              NaN                          NaN   \n",
      "3                              NaN                          NaN   \n",
      "4                              NaN  Odonnell-Gage@bellsouth.net   \n",
      "\n",
      "                         from   size  attachments  \\\n",
      "0   Lynn.Adena.Pratt@dtaa.com  25830            0   \n",
      "1         MOH68@optonline.net  29942            0   \n",
      "2  Lynn_A_Pratt@earthlink.net  28780            0   \n",
      "3  Lynn_A_Pratt@earthlink.net  21907            0   \n",
      "4         MOH68@optonline.net  17319            0   \n",
      "\n",
      "                                             content  Threat  \n",
      "0  middle f2 systems 4 july techniques powerful d...       0  \n",
      "1  the breaking called allied reservations former...       1  \n",
      "2  slowly this uncinus winter beneath addition ex...       0  \n",
      "3  400 other difficult land cirrocumulus powered ...       0  \n",
      "4  this kmh october holliswood number advised unu...       1  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv(os.path.join(path, \"email.csv\"))\n",
    "# Map Threat column to numerical (Yes -> 1, No -> 0)\n",
    "df[\"Threat\"] = np.random.choice([\"Yes\", \"No\"], size=len(df), p=[0.5, 0.5])\n",
    "df[\"Threat\"] = df[\"Threat\"].map({\"Yes\": 1, \"No\": 0})\n",
    "# Display first few rows\n",
    "print(df.head(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Shape: (2629979, 12)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2629979 entries, 0 to 2629978\n",
      "Data columns (total 12 columns):\n",
      " #   Column       Dtype \n",
      "---  ------       ----- \n",
      " 0   id           object\n",
      " 1   date         object\n",
      " 2   user         object\n",
      " 3   pc           object\n",
      " 4   to           object\n",
      " 5   cc           object\n",
      " 6   bcc          object\n",
      " 7   from         object\n",
      " 8   size         int64 \n",
      " 9   attachments  int64 \n",
      " 10  content      object\n",
      " 11  Threat       int64 \n",
      "dtypes: int64(3), object(9)\n",
      "memory usage: 240.8+ MB\n",
      "None\n",
      "               size   attachments        Threat\n",
      "count  2.629979e+06  2.629979e+06  2.629979e+06\n",
      "mean   2.999232e+04  4.035960e-01  5.003268e-01\n",
      "std    9.993642e+03  1.049910e+00  5.000000e-01\n",
      "min    6.182000e+03  0.000000e+00  0.000000e+00\n",
      "25%    2.285900e+04  0.000000e+00  0.000000e+00\n",
      "50%    2.845500e+04  0.000000e+00  1.000000e+00\n",
      "75%    3.541800e+04  0.000000e+00  1.000000e+00\n",
      "max    1.419090e+05  9.000000e+00  1.000000e+00\n",
      "id                   0\n",
      "date                 0\n",
      "user                 0\n",
      "pc                   0\n",
      "to                   0\n",
      "cc             1617054\n",
      "bcc            2212977\n",
      "from                 0\n",
      "size                 0\n",
      "attachments          0\n",
      "content              0\n",
      "Threat               0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Dataset Shape:\", df.shape)\n",
    "\n",
    "print(df.info())  # Overview of columns and data types\n",
    "print(df.describe())  # Summary statistics\n",
    "print(df.isnull().sum())  # Count of missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\pc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download(\"stopwords\")\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "# Function to preprocess text\n",
    "def preprocess_text(text):\n",
    "    if not isinstance(text, str):  # Handle missing or non-string values\n",
    "        return \"\"\n",
    "    \n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = re.sub(r\"\\d+\", \"\", text)  # Remove numbers\n",
    "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))  # Remove punctuation\n",
    "    text = \" \".join(word for word in text.split() if word not in stop_words)  # Remove stopwords\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample = df.sample(n=20000, random_state=42)  # Use 10,000 samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply text preprocessing\n",
    "df_sample[\"content\"] = df_sample[\"content\"].astype(str)\n",
    "df_sample[\"clean_content\"] = df_sample[\"content\"].apply(preprocess_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(max_features=1000)\n",
    "X_text = vectorizer.fit_transform(df_sample[\"clean_content\"])\n",
    "\n",
    "# Convert to DataFrame with actual words as column names\n",
    "X_text_df = pd.DataFrame(X_text.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "X_text_df = X_text_df.reset_index(drop=True)\n",
    "df_features = df_sample[[\"size\", \"attachments\"]].reset_index(drop=True)\n",
    "\n",
    "X = pd.concat([X_text_df, df_features], axis=1)\n",
    "y = df_sample[\"Threat\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000 20000\n"
     ]
    }
   ],
   "source": [
    "print(len(X), len(y))  # They should be the same\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train model and return accuracy\n",
    "def train_and_evaluate(X_train, X_test, y_train, y_test):\n",
    "    model = LogisticRegression(max_iter=1000)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    allresults={ \"accuracy\":accuracy_score(y_test, y_pred),\n",
    "                 \"precision\":precision_score(y_test, y_pred),\n",
    "                 \"recall\":recall_score(y_test, y_pred),    \n",
    "                 \" f1\":f1_score(y_test, y_pred)\n",
    "                }\n",
    "    return allresults\n",
    "# Baseline Model (Raw Data)\n",
    "accuracy_baseline = train_and_evaluate(X_train, X_test, y_train, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_and_evaluate(X_train, X_test, y_train, y_test):\n",
    "    model = LogisticRegression(max_iter=1000)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    allresults={ \"accuracy\":accuracy_score(y_test, y_pred),\n",
    "                 \"precision\":precision_score(y_test, y_pred),\n",
    "                 \"recall\":recall_score(y_test, y_pred),    \n",
    "                  \"f1\":f1_score(y_test, y_pred)\n",
    "                }\n",
    "    return allresults\n",
    "\n",
    "#after encoding\n",
    "accuracy_encoding = train_and_evaluate(X_train, X_test, y_train, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardization\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "accuracy_standardized = train_and_evaluate(X_train_scaled, X_test_scaled, y_train, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pc\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:776: UserWarning: k=3000 is greater than n_features=1002. All the features will be returned.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Feature Selection (Select best 3000 features)\n",
    "selector = SelectKBest(chi2, k=3000)\n",
    "X_train_selected = selector.fit_transform(X_train, y_train)\n",
    "X_test_selected = selector.transform(X_test)\n",
    "accuracy_feature_selection = train_and_evaluate(X_train_selected, X_test_selected, y_train, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SMOTE (Handle Imbalanced Data)\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
    "accuracy_smote = train_and_evaluate(X_train_smote, X_test, y_train_smote, y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA (Reduce to 300 components)\n",
    "pca = PCA(n_components=300)\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)\n",
    "accuracy_pca = train_and_evaluate(X_train_pca, X_test_pca, y_train, y_test)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
